{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoELECTRA + KBMC Baseline Training\n",
    "\n",
    "**목적**: 한국어 의료 NER baseline 구축 (GLiNER 비교용)\n",
    "\n",
    "| 항목 | 내용 |\n",
    "|------|------|\n",
    "| 모델 | monologg/koelectra-base-v3-discriminator |\n",
    "| 데이터 | SungJoo/KBMC (6,150 문장) |\n",
    "| 라벨 | Disease, Body, Treatment (BIO) |\n",
    "| 목표 | F1 ~98% (논문 수준) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 패키지 설치\n",
    "!pip install -q transformers datasets accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 학습 설정\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "OUTPUT_DIR = \"./koelectra-kbmc-baseline\"\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16  # GPU면 16, CPU면 8로 조정\n",
    "MAX_LENGTH = 128\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# 라벨 정의 (KBMC 데이터셋 기준)\n",
    "LABEL_LIST = ['O', 'Disease-B', 'Disease-I', 'Body-B', 'Body-I', 'Treatment-B', 'Treatment-I']\n",
    "LABEL2ID = {label: i for i, label in enumerate(LABEL_LIST)}\n",
    "ID2LABEL = {i: label for i, label in enumerate(LABEL_LIST)}\n",
    "\n",
    "print(f\"Labels: {LABEL_LIST}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KBMC 데이터셋 로드\n",
    "dataset = load_dataset(\"SungJoo/KBMC\")\n",
    "print(f\"전체 데이터: {len(dataset['train'])} 문장\")\n",
    "\n",
    "# 샘플 확인\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\n샘플:\")\n",
    "print(f\"  Sentence: {sample['Sentence'][:50]}...\")\n",
    "print(f\"  Tags: {sample['Tags'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test 분리 (90/10)\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "print(f\"Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 토크나이저 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"KBMC 데이터를 토크나이저에 맞게 정렬\"\"\"\n",
    "    \n",
    "    # 토큰과 태그 분리\n",
    "    all_tokens = [sent.split() for sent in examples['Sentence']]\n",
    "    all_tags = [tags.split() for tags in examples['Tags']]\n",
    "    \n",
    "    # 토크나이징\n",
    "    tokenized_inputs = tokenizer(\n",
    "        all_tokens,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, (tokens, tags) in enumerate(zip(all_tokens, all_tags)):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # 특수 토큰 ([CLS], [SEP], [PAD])\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # 새로운 단어의 첫 번째 토큰\n",
    "                if word_idx < len(tags):\n",
    "                    label_ids.append(LABEL2ID.get(tags[word_idx], 0))\n",
    "                else:\n",
    "                    label_ids.append(0)  # O 태그\n",
    "            else:\n",
    "                # 같은 단어의 서브워드\n",
    "                if word_idx < len(tags):\n",
    "                    current_tag = tags[word_idx]\n",
    "                    # B- 태그를 I- 태그로 변환\n",
    "                    if current_tag.endswith('-B'):\n",
    "                        i_tag = current_tag.replace('-B', '-I')\n",
    "                        label_ids.append(LABEL2ID.get(i_tag, 0))\n",
    "                    else:\n",
    "                        label_ids.append(LABEL2ID.get(current_tag, 0))\n",
    "                else:\n",
    "                    label_ids.append(0)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 적용\n",
    "print(\"전처리 중...\")\n",
    "tokenized_train = dataset['train'].map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "tokenized_test = dataset['test'].map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['test'].column_names\n",
    ")\n",
    "\n",
    "print(f\"전처리 완료: Train {len(tokenized_train)}, Test {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL_LIST),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ")\n",
    "\n",
    "# GPU로 이동\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    \n",
    "print(f\"모델 로드 완료: {MODEL_NAME}\")\n",
    "print(f\"파라미터 수: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # -100 제외하고 실제 라벨만 추출\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        for p, l in zip(pred, label):\n",
    "            if l != -100:\n",
    "                true_labels.append(l)\n",
    "                true_predictions.append(p)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, true_predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),  # GPU면 FP16 사용\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"학습 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "print(\"=\" * 50)\n",
    "print(\"학습 시작\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 평가\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"최종 평가 결과\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1:        {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall:    {eval_results['eval_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/final\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")\n",
    "print(f\"모델 저장: {OUTPUT_DIR}/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(text, model, tokenizer):\n",
    "    \"\"\"단일 문장 NER 추론\"\"\"\n",
    "    tokens = text.split()\n",
    "    inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "    word_ids = inputs.word_ids() if hasattr(inputs, 'word_ids') else None\n",
    "    \n",
    "    # 결과 매핑\n",
    "    results = []\n",
    "    prev_word_idx = None\n",
    "    for idx, (token_id, pred) in enumerate(zip(inputs['input_ids'][0], predictions)):\n",
    "        word_idx = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
    "        if word_idx not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            label = ID2LABEL[pred.item()]\n",
    "            results.append((word_idx, label))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 테스트\n",
    "test_text = \"전신 적 다한증 은 신체 전체 에 힘 이 빠져서 일상 생활 이 어려워 지는 질환 으로\"\n",
    "results = predict_ner(test_text, model, tokenizer)\n",
    "\n",
    "print(\"추론 테스트:\")\n",
    "print(f\"입력: {test_text}\")\n",
    "print(f\"\\n결과:\")\n",
    "for token, label in results[:20]:  # 처음 20개만\n",
    "    if label != 'O':\n",
    "        print(f\"  {token}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 요약\n",
    "summary = f\"\"\"\n",
    "# KoELECTRA + KBMC Baseline 결과\n",
    "\n",
    "| 항목 | 값 |\n",
    "|------|----|\n",
    "| 모델 | {MODEL_NAME} |\n",
    "| 데이터셋 | SungJoo/KBMC |\n",
    "| Train | {len(dataset['train'])} |\n",
    "| Test | {len(dataset['test'])} |\n",
    "| Epochs | {EPOCHS} |\n",
    "| Batch Size | {BATCH_SIZE} |\n",
    "\n",
    "## 평가 결과\n",
    "\n",
    "| 지표 | 값 |\n",
    "|------|----|\n",
    "| **F1** | **{eval_results['eval_f1']:.4f}** |\n",
    "| Precision | {eval_results['eval_precision']:.4f} |\n",
    "| Recall | {eval_results['eval_recall']:.4f} |\n",
    "| Accuracy | {eval_results['eval_accuracy']:.4f} |\n",
    "\n",
    "## 비교 (GLiNER zero-shot)\n",
    "\n",
    "| 모델 | F1 |\n",
    "|------|----|  \n",
    "| KoELECTRA (이 결과) | {eval_results['eval_f1']:.4f} |\n",
    "| GLiNER zero-shot | ~0.20-0.30 (추정) |\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# 파일로 저장\n",
    "with open(f\"{OUTPUT_DIR}/results.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Google Drive 저장 (Colab용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab에서 Google Drive에 저장하려면 실행\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp -r {OUTPUT_DIR} /content/drive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
