# GLiNER2 LoRA 파인튜닝 테스트 결과

> 테스트일: 2026-01-06

---

## 1. 테스트 개요

### 목적
GLiNER2의 LoRA 파인튜닝으로 한국어 의료 NER 성능을 개선할 수 있는지 검증

### 가설
> LoRA 파인튜닝으로 한국어 의료 도메인에 적응하면, GLiNER2의 구조적 장점을 유지하면서 한국어 성능을 개선할 수 있다.

---

## 2. 실험 환경

### 하드웨어
| 항목 | 값 |
|------|-----|
| GPU | ❌ CPU only |
| 실행 시간 | 2분 30초 |
| 처리 속도 | 2.6 samples/sec |

### 모델
| 항목 | 값 |
|------|-----|
| 베이스 모델 | `fastino/gliner2-base-v1` |
| 인코더 | DeBERTa-v3-base (205M params) |
| LoRA params | 3,096,592 (1.46%) |

### 데이터
| 항목 | 값 |
|------|-----|
| 데이터셋 | SungJoo/KBMC |
| Train | 37개 |
| Test | 8개 |
| 라벨 | Disease, Body, Treatment |

### LoRA 설정
```python
use_lora = True
lora_r = 16
lora_alpha = 32
lora_dropout = 0.1
lora_target_modules = ["encoder", "span_rep", "classifier"]
num_epochs = 10
batch_size = 4
task_lr = 5e-4
```

---

## 3. 결과

### 3.1 정량적 결과

| 지표 | 베이스라인 | 파인튜닝 후 | Delta |
|------|-----------|------------|-------|
| Precision | 0.5000 | 0.0000 | -0.5000 |
| Recall | 0.0455 | 0.0000 | -0.0455 |
| **F1** | **0.0833** | **0.0000** | **-0.0833** |
| TP | 1 | 0 | -1 |
| FP | 1 | 0 | -1 |
| FN | 21 | 22 | +1 |

### 3.2 Training Loss 추이

| Epoch | Loss |
|-------|------|
| 1 | 7.5144 |
| 2 | 0.6174 |
| 3 | 0.4563 |
| 4 | 1.6171 |
| 5 | 0.9600 |
| 6 | 0.0151 |
| 7 | 0.0093 |
| 8 | 0.0000 |
| 9 | 0.0023 |
| 10 | 0.2200 |

**관찰**: Loss는 0에 수렴했으나 테스트 성능은 오히려 하락

---

## 4. 분석

### 4.1 실패 원인

#### 1) 과적합 (Overfitting)
- 37개 학습 데이터로 10 epoch 학습
- Loss 0 수렴 = 학습 데이터 암기
- 테스트 데이터에 일반화 실패

#### 2) 인코더 한계
- DeBERTa-v3-base는 영어 중심 토크나이저
- 한국어 토큰화가 비효율적 (음절 단위 분리)
- 예: "당뇨병" → ['▁', '당', '뇨', '병']
- LoRA는 인코더 위에서 학습하므로 근본적 한계 해결 불가

#### 3) 데이터 불균형
- 테스트셋 8개로 통계적 유의성 부족
- 일부 라벨 데이터 누락 가능성

### 4.2 베이스라인 vs GLiNER v2.1 비교

| 모델 | 인코더 | 한국어 F1 (추정) |
|------|--------|-----------------|
| GLiNER2 (base) | DeBERTa-v3-base | ~8% |
| GLiNER2 + LoRA | DeBERTa-v3-base | 0% (과적합) |
| GLiNER v2.1 | XLM-RoBERTa | ~65% (이전 테스트) |

---

## 5. 결론

### 5.1 테스트 결과
- **LoRA만으로는 한국어 성능 개선 불가**
- 인코더 자체가 한국어를 제대로 처리하지 못함
- 과적합 문제도 심각

### 5.2 권장 사항

| 우선순위 | 방안 | 예상 효과 |
|----------|------|----------|
| 1 | **인코더 교체** (KLUE-RoBERTa, KoELECTRA) | 높음 |
| 2 | 학습 데이터 대폭 확대 (1,000개+) | 중간 |
| 3 | GLiNER v2.1 사용 (다국어 모델) | 높음 (검증됨) |

### 5.3 GLiNER2 구조의 가치

LoRA 테스트는 실패했으나, GLiNER2의 구조적 장점은 여전히 유효:
- 스키마 기반 인터페이스 (엔티티 설명 포함)
- 다중 태스크 지원 (NER + Classification + Relation)
- CPU 최적화

→ **인코더만 한국어용으로 교체하면 활용 가치 있음**

---

## 6. 다음 단계

### Option A: 인코더 교체 후 풀 파인튜닝
- 한국어 인코더 (KLUE-RoBERTa, KoELECTRA) 사용
- GLiNER2 구조는 유지
- 대규모 데이터로 처음부터 학습

### Option B: GLiNER v2.1 활용
- 이미 다국어 지원 (한국어 성능 검증됨)
- 스키마 인터페이스는 포기
- 빠른 적용 가능

### Option C: 하이브리드
- 추론: GLiNER v2.1 (다국어)
- 후처리: GLiNER2 스타일 스키마 적용

---

## 7. 관련 파일

| 파일 | 설명 |
|------|------|
| `scripts/gliner2/train_lora_kbmc.py` | 테스트 스크립트 |
| `docs/plans/gliner2_lora_finetuning_plan.md` | 테스트 계획 |
| `docs/study/gliner2_test_results.md` | 사전 테스트 결과 |
